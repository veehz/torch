{% macro kernel_options() -%}
  {}
{%- endmacro %}

{% macro binary_op_base(name, operation) -%}
const _{{ name }}_kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = {{ operation }};
  }
  return res;
};

function _{{ name }}_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = _{{ name }}_kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
{%- endmacro %}

{% macro unary_op_base(name, operation) -%}
// function generated from unary_op_base("{{ name }}", "{{ operation }}")

const _{{ name }}_kernel = function (a: number[], output: number) {
  const res = new Array(output);
  for (let x = 0; x < output; x++) {
    res[x] = {{ operation }};
  }
  return res;
};

function _{{ name }}_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = _{{ name }}_kernel;
  const output = a.shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, output) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
{%- endmacro %}

{% macro binary_op_class(classname, opname, backward_operations) -%}
// class generated from binary_op_class("{{ classname }}", "{{ opname }}", backward_operations)
export class {{ classname }} extends BinaryOperation {
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.saved_tensors = [a, b];
    }
    this.next_functions.push(a.grad_fn ? a.grad_fn : nullOp);
    this.next_functions.push(b.grad_fn ? b.grad_fn : nullOp);
    return _{{ opname }}_tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.saved_tensors;
    const [aFn, bFn] = this.next_functions;

    // backward_operations:
    {{ backward_operations | join("\n    ") }}
  }
}
registerOperation('{{ opname }}', {{ classname }});
{%- endmacro %}

{% macro unary_op_class(classname, opname, backward_operations) -%}
// class generated from unary_op_class("{{ classname }}", "{{ opname }}", backward_operations)
export class {{ classname }} extends UnaryOperation {
  protected _forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.saved_tensors = [a];
    }
    this.next_functions.push(a.grad_fn ? a.grad_fn : nullOp);
    return _{{ opname }}_tensor(a, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.saved_tensors;
    const [aFn] = this.next_functions;

    // backward_operations:
    {{ backward_operations | join("\n    ") }}
  }
}
registerOperation('{{ opname }}', {{ classname }});
{%- endmacro %}

{% macro powint_op_base() -%}
function _powint_tensor(a: Tensor, n: number, operation: Operation | null = null): Tensor {
  const data = new Array(a.dataLength());
  for (let i = 0; i < data.length; i++) {
    data[i] = Math.pow(a.data[i], n);
  }
  return new Tensor(
    data,
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
{%- endmacro %}

{% macro powint_op_class() -%}
export class PowInt extends Operation {
  private n: number;
  protected _forward(a: Tensor, n: number): Tensor {
    if (a.requires_grad) {
      this.saved_tensors = [a];
      this.n = n;
    }

    this.next_functions.push(a.grad_fn ? a.grad_fn : nullOp);
    return _powint_tensor(a, n, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.saved_tensors;
    const n = this.n;
    const [aFn] = this.next_functions;

    // backward_operations:
    aFn.backward(dz.mul(n).mul(a.pow(n - 1)));
  }
}
registerOperation('powint', PowInt);
{%- endmacro %}

{% macro transpose_op_base() -%}
function _transpose_tensor(
  a: Tensor,
  dim0: number,
  dim1: number,
  operation: Operation | null = null
): Tensor {
  const output_shape = [...a.shape];
  [output_shape[dim0], output_shape[dim1]] = [output_shape[dim1], output_shape[dim0]];
  const size = a.dataLength();
  const data = new Array(size);

  const a_strides = new Array(a.shape.length);
  const out_strides = new Array(output_shape.length);
  for (let i = a.shape.length - 1, s = 1; i >= 0; i--) {
    a_strides[i] = s;
    s *= a.shape[i];
  }
  for (let i = output_shape.length - 1, s = 1; i >= 0; i--) {
    out_strides[i] = s;
    s *= output_shape[i];
  }

  for(let i=0; i<size; i++) {
    let idx = i;
    let input_idx = 0;
    for (let d = 0; d < output_shape.length; d++) {
      const stride = out_strides[d];
      const coord = Math.floor(idx / stride);
      idx %= stride;

      let input_d = d;
      if (d === dim0) input_d = dim1;
      else if (d === dim1) input_d = dim0;

      input_idx += coord * a_strides[input_d];
    }
    data[i] = a.data[input_idx];
  }

  return new Tensor(
    data,
    { requires_grad: a.requires_grad },
    { operation: operation, shape: output_shape }
  );
}
{%- endmacro %}

{% macro transpose_op_class() -%}
export class Transpose extends Operation {
  private dim0: number;
  private dim1: number;
  protected _forward(a: Tensor, dim0: number, dim1: number): Tensor {
    if (a.requires_grad) {
        this.saved_tensors = [a];
        this.dim0 = dim0;
        this.dim1 = dim1;
    }
    this.next_functions.push(a.grad_fn ? a.grad_fn : nullOp);
    return _transpose_tensor(a, dim0, dim1, this);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.saved_tensors;
    const dim0 = this.dim0;
    const dim1 = this.dim1;
    const [aFn] = this.next_functions;

    // backward_operations:
    aFn.backward(dz.transpose(dim0, dim1));
  }
}
registerOperation('transpose', Transpose);
{%- endmacro %}

{% macro matmul_op_base() -%}
function _matmul_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  if (a.shape.length == 1 && b.shape.length == 1) {
    return a.mul(b).sum();
  }

  const a_1d = a.shape.length == 1;
  const b_1d = b.shape.length == 1;

  const a_shape = a_1d ? [1, a.shape[0]] : a.shape;
  const b_shape = b_1d ? [b.shape[0], 1] : b.shape;

  if (a_shape[a_shape.length - 1] != b_shape[b_shape.length - 2]) {
    throw new Error('Shape mismatch: ' + a.shape + ' and ' + b.shape);
  }

  const broadcast_shape = _broadcast_shape(a_shape.slice(0, -2), b_shape.slice(0, -2)).concat([
    a_shape[a_shape.length - 2],
    b_shape[b_shape.length - 1]
  ]);

  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);
  const data = new Array(output_size).fill(0);

  const padded_a_shape = _pad_shape(a_shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b_shape, broadcast_shape);

  const dim_M = broadcast_shape[broadcast_shape.length - 2];
  const dim_N = broadcast_shape[broadcast_shape.length - 1];
  const dim_K = a_shape[a_shape.length - 1]; // or b_shape[b_shape.length - 2]

  for (let i = 0; i < output_size; i++) {
    const mn_idx = i % (dim_M * dim_N);
    const m = Math.floor(mn_idx / dim_N);
    const n = mn_idx % dim_N;

    let base_a = _get_original_index(padded_a_shape, broadcast_shape, i - n);
    let base_b = _get_original_index(padded_b_shape, broadcast_shape, i - m * dim_N);

    let sum = 0;
    for(let k=0; k < dim_K; k++) {
      sum += a.data[base_a + k] * b.data[base_b + k * dim_N];
    }
    data[i] = sum;
  }

  let shape_after_removing_extra_dims = [...broadcast_shape];

  if (a_1d) {
    shape_after_removing_extra_dims = shape_after_removing_extra_dims
      .slice(0, -2)
      .concat([broadcast_shape[broadcast_shape.length - 1]]);
  }

  if (b_1d) {
    shape_after_removing_extra_dims = shape_after_removing_extra_dims.slice(0, -1);
  }

  return new Tensor(
    data,
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: shape_after_removing_extra_dims }
  );
}
{%- endmacro %}