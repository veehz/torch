{% macro binary_op_base(name, operation) -%}
// function generated from binary_op_base("{{ name }}", "{{ operation }}")
function _{{ name }}_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);
  const kernel = gpu.createKernel(
    function (a: number[], as: number[], b: number[], bs: number[], bcs: number[]) {
      const a_index = _get_original_index_kernel(as, bcs, this.thread.x);
      const b_index = _get_original_index_kernel(bs, bcs, this.thread.x);

      return {{ operation }};
    },
    {
      constants: {
        shape_length: broadcast_shape.length
      },
      output: [broadcast_shape.reduce((acc, val) => acc * val, 1)]
    }
  );

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
{%- endmacro %}

{% macro unary_op_base(name, operation) -%}
// function generated from unary_op_base("{{ name }}", "{{ operation }}")
function _{{ name }}_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = gpu.createKernel(
    function (a: number[]) {
      return {{ operation }};
    },
    {
      output: [a.shape.reduce((acc, val) => acc * val, 1)]
    }
  );

  return new Tensor(
    kernel(a.data) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
{%- endmacro %}

{% macro binary_op_class(classname, opname, backward_operations) -%}
// class generated from binary_op_class("{{ classname }}", "{{ opname }}", backward_operations)
export class {{ classname }} extends BinaryOperation {
  private cache: [Tensor, Tensor];
  public forward(a: Tensor, b: Tensor): Tensor {
    this.cache = [a, b];
    return _{{ opname }}_tensor(a, b, this);
  }
  public backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    {{ backward_operations | join("\n    ") }}
  }
}
registerOperation('{{ opname }}', {{ classname }});
{%- endmacro %}

{% macro unary_op_class(classname, opname, backward_operations) -%}
// class generated from unary_op_class("{{ classname }}", "{{ opname }}", backward_operations)
export class {{ classname }} extends UnaryOperation {
  private cache: [Tensor];
  public forward(a: Tensor): Tensor {
    this.cache = [a];
    return _{{ opname }}_tensor(a, this);
  }
  public backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    {{ backward_operations | join("\n    ") }}
  }
}
registerOperation('{{ opname }}', {{ classname }});
{%- endmacro %}