// This file is generated by scripts/generate_script.py from src/operations/ops.ts.j2
import { Tensor } from '../tensor';
import {
  _broadcast_shape,
  _get_original_index_from_transposed_index,
  _get_original_index,
  _get_original_index_kernel,
  _pad_shape
} from '../broadcasting';
// import gpu, { Texture } from '../gpu';
import { Operation, BinaryOperation, UnaryOperation } from './base';
import * as functional from './functional';
import { registerOperation } from './registry';

// debug operations

const ___left_index___kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = a_index;
  }
  return res;
};

function ___left_index___tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = ___left_index___kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("__Left_index__", "__left_index__", backward_operations)
export class __Left_index__ extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return ___left_index___tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    
  }
}
registerOperation('__left_index__', __Left_index__);

const ___right_index___kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = b_index;
  }
  return res;
};

function ___right_index___tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = ___right_index___kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("__Right_index__", "__right_index__", backward_operations)
export class __Right_index__ extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return ___right_index___tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    
  }
}
registerOperation('__right_index__', __Right_index__);

// binary pointwise

const _add_kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = a[a_index] + b[b_index];
  }
  return res;
};

function _add_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = _add_kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Add", "add", backward_operations)
export class Add extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return _add_tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    a.backward(dz);
    b.backward(dz);
  }
}
registerOperation('add', Add);

const _sub_kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = a[a_index] - b[b_index];
  }
  return res;
};

function _sub_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = _sub_kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Sub", "sub", backward_operations)
export class Sub extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return _sub_tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    a.backward(dz);
    b.backward(dz.mul(new Tensor(-1)));
  }
}
registerOperation('sub', Sub);

const _mul_kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = a[a_index] * b[b_index];
  }
  return res;
};

function _mul_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = _mul_kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Mul", "mul", backward_operations)
export class Mul extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return _mul_tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    a.backward(dz.mul(b));
    b.backward(dz.mul(a));
  }
}
registerOperation('mul', Mul);

const _div_kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = a[a_index] / b[b_index];
  }
  return res;
};

function _div_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = _div_kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Div", "div", backward_operations)
export class Div extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return _div_tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    a.backward(dz.div(b));
    b.backward(dz.mul(a).mul(new Tensor(-1)).div(b).div(b));
  }
}
registerOperation('div', Div);

const _pow_kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = Math.pow(a[a_index], b[b_index]);
  }
  return res;
};

function _pow_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = _pow_kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Pow", "pow", backward_operations)
export class Pow extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return _pow_tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    a.backward(dz.mul(b).mul(a.pow(b.sub(new Tensor(1)))));
    b.backward(dz.mul(a.pow(b)).mul(a.log()));
  }
}
registerOperation('pow', Pow);

const _fmod_kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = a[a_index] % b[b_index];
  }
  return res;
};

function _fmod_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = _fmod_kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Fmod", "fmod", backward_operations)
export class Fmod extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return _fmod_tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    a.backward(dz);
  }
}
registerOperation('fmod', Fmod);

const _maximum_kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = Math.max(a[a_index], b[b_index]);
  }
  return res;
};

function _maximum_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = _maximum_kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Maximum", "maximum", backward_operations)
export class Maximum extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return _maximum_tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    a.backward(dz.mul(a.ge(b)));
    b.backward(dz.mul(b.gt(a)));
  }
}
registerOperation('maximum', Maximum);

const _minimum_kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = Math.min(a[a_index], b[b_index]);
  }
  return res;
};

function _minimum_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = _minimum_kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Minimum", "minimum", backward_operations)
export class Minimum extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return _minimum_tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    a.backward(dz.mul(a.le(b)));
    b.backward(dz.mul(b.lt(a)));
  }
}
registerOperation('minimum', Minimum);


function _powint_tensor(a: Tensor, n: number, operation: Operation | null = null): Tensor {
  const data = new Array(a.dataLength());
  for (let i = 0; i < data.length; i++) {
    data[i] = Math.pow(a.data[i], n);
  }
  return new Tensor(
    data,
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
export class PowInt extends Operation {
  private cache: [Tensor, number];
  protected _forward(a: Tensor, n: number): Tensor {
    if (a.requires_grad) {
      this.cache = [a, n];
    }

    return _powint_tensor(a, n, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, n] = this.cache;

    // backward_operations:
    a.backward(dz.mul(n).mul(a.pow(n - 1)));
  }
}
registerOperation('powint', PowInt);

// unary pointwise

// function generated from unary_op_base("log", "Math.log(a[x])")

const _log_kernel = function (a: number[], output: number) {
  const res = new Array(output);
  for (let x = 0; x < output; x++) {
    res[x] = Math.log(a[x]);
  }
  return res;
};

function _log_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = _log_kernel;
  const output = a.shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, output) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Log", "log", backward_operations)
export class Log extends UnaryOperation {
  private cache: [Tensor];
  protected _forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _log_tensor(a, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    a.backward(new Tensor(1).div(a));
  }
}
registerOperation('log', Log);

// function generated from unary_op_base("sqrt", "Math.sqrt(a[x])")

const _sqrt_kernel = function (a: number[], output: number) {
  const res = new Array(output);
  for (let x = 0; x < output; x++) {
    res[x] = Math.sqrt(a[x]);
  }
  return res;
};

function _sqrt_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = _sqrt_kernel;
  const output = a.shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, output) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Sqrt", "sqrt", backward_operations)
export class Sqrt extends UnaryOperation {
  private cache: [Tensor];
  protected _forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _sqrt_tensor(a, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    a.backward(new Tensor(1).div(a.sqrt()).div(2));
  }
}
registerOperation('sqrt', Sqrt);

// function generated from unary_op_base("exp", "Math.exp(a[x])")

const _exp_kernel = function (a: number[], output: number) {
  const res = new Array(output);
  for (let x = 0; x < output; x++) {
    res[x] = Math.exp(a[x]);
  }
  return res;
};

function _exp_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = _exp_kernel;
  const output = a.shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, output) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Exp", "exp", backward_operations)
export class Exp extends UnaryOperation {
  private cache: [Tensor];
  protected _forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _exp_tensor(a, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    a.backward(dz.mul(a.exp()));
  }
}
registerOperation('exp', Exp);

// function generated from unary_op_base("square", "a[x] * a[x]")

const _square_kernel = function (a: number[], output: number) {
  const res = new Array(output);
  for (let x = 0; x < output; x++) {
    res[x] = a[x] * a[x];
  }
  return res;
};

function _square_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = _square_kernel;
  const output = a.shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, output) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Square", "square", backward_operations)
export class Square extends UnaryOperation {
  private cache: [Tensor];
  protected _forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _square_tensor(a, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    a.backward(dz.mul(a).mul(new Tensor(2)));
  }
}
registerOperation('square', Square);

// function generated from unary_op_base("abs", "Math.abs(a[x])")

const _abs_kernel = function (a: number[], output: number) {
  const res = new Array(output);
  for (let x = 0; x < output; x++) {
    res[x] = Math.abs(a[x]);
  }
  return res;
};

function _abs_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = _abs_kernel;
  const output = a.shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, output) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Abs", "abs", backward_operations)
export class Abs extends UnaryOperation {
  private cache: [Tensor];
  protected _forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _abs_tensor(a, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    a.backward(dz.mul(functional.sign(a)));
  }
}
registerOperation('abs', Abs);

// function generated from unary_op_base("sign", "Math.sign(a[x])")

const _sign_kernel = function (a: number[], output: number) {
  const res = new Array(output);
  for (let x = 0; x < output; x++) {
    res[x] = Math.sign(a[x]);
  }
  return res;
};

function _sign_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = _sign_kernel;
  const output = a.shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, output) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Sign", "sign", backward_operations)
export class Sign extends UnaryOperation {
  private cache: [Tensor];
  protected _forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _sign_tensor(a, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    
  }
}
registerOperation('sign', Sign);

// function generated from unary_op_base("neg", "-a[x]")

const _neg_kernel = function (a: number[], output: number) {
  const res = new Array(output);
  for (let x = 0; x < output; x++) {
    res[x] = -a[x];
  }
  return res;
};

function _neg_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = _neg_kernel;
  const output = a.shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, output) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Neg", "neg", backward_operations)
export class Neg extends UnaryOperation {
  private cache: [Tensor];
  protected _forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _neg_tensor(a, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    a.backward(dz.mul(new Tensor(-1)));
  }
}
registerOperation('neg', Neg);

// function generated from unary_op_base("reciprocal", "1 / a[x]")

const _reciprocal_kernel = function (a: number[], output: number) {
  const res = new Array(output);
  for (let x = 0; x < output; x++) {
    res[x] = 1 / a[x];
  }
  return res;
};

function _reciprocal_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = _reciprocal_kernel;
  const output = a.shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, output) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Reciprocal", "reciprocal", backward_operations)
export class Reciprocal extends UnaryOperation {
  private cache: [Tensor];
  protected _forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _reciprocal_tensor(a, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    a.backward(dz.mul(a.pow(-2)));
  }
}
registerOperation('reciprocal', Reciprocal);

export class Reshape extends Operation {
  private cache: [Tensor];
  protected _forward(a: Tensor, shape: number[]) {
    const previous_length = a.dataLength();
    const target_length = shape.reduce((acc, val) => acc * val, 1);

    if (previous_length !== target_length) {
      throw new Error('Shape mismatch: ' + a.shape + ' and ' + shape);
    }

    if (a.requires_grad) {
      this.cache = [a];
    }

    return new Tensor(
      a.data,
      { requires_grad: a.requires_grad },
      { operation: a.requires_grad ? this : null, shape }
    );
  }
  protected _backward(dz: Tensor) {
    const [a] = this.cache;

    // backward_operations:
    a.backward(dz.reshape(a.shape));
  }
}
registerOperation('reshape', Reshape);

export class Unsqueeze extends Operation {
  private cache: [Tensor];
  protected _forward(a: Tensor, dim: number) {
    if (a.requires_grad) {
      this.cache = [a];
    }

    if (dim < 0) {
      dim += a.shape.length + 1;
    }

    const shape = [...a.shape];
    shape.splice(dim, 0, 1);

    return new Tensor(
      a.data,
      { requires_grad: a.requires_grad },
      { operation: a.requires_grad ? this : null, shape }
    );
  }
  protected _backward(dz: Tensor) {
    const [a] = this.cache;

    // backward_operations:
    a.backward(dz.reshape(a.shape));
  }
}
registerOperation('unsqueeze', Unsqueeze);

// trigonometric

// function generated from unary_op_base("sin", "Math.sin(a[x])")

const _sin_kernel = function (a: number[], output: number) {
  const res = new Array(output);
  for (let x = 0; x < output; x++) {
    res[x] = Math.sin(a[x]);
  }
  return res;
};

function _sin_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = _sin_kernel;
  const output = a.shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, output) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Sin", "sin", backward_operations)
export class Sin extends UnaryOperation {
  private cache: [Tensor];
  protected _forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _sin_tensor(a, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    a.backward(dz.mul(a.cos()));
  }
}
registerOperation('sin', Sin);

// function generated from unary_op_base("cos", "Math.cos(a[x])")

const _cos_kernel = function (a: number[], output: number) {
  const res = new Array(output);
  for (let x = 0; x < output; x++) {
    res[x] = Math.cos(a[x]);
  }
  return res;
};

function _cos_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = _cos_kernel;
  const output = a.shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, output) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Cos", "cos", backward_operations)
export class Cos extends UnaryOperation {
  private cache: [Tensor];
  protected _forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _cos_tensor(a, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    a.backward(dz.mul(a.sin().neg()));
  }
}
registerOperation('cos', Cos);

// function generated from unary_op_base("tan", "Math.tan(a[x])")

const _tan_kernel = function (a: number[], output: number) {
  const res = new Array(output);
  for (let x = 0; x < output; x++) {
    res[x] = Math.tan(a[x]);
  }
  return res;
};

function _tan_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = _tan_kernel;
  const output = a.shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, output) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Tan", "tan", backward_operations)
export class Tan extends UnaryOperation {
  private cache: [Tensor];
  protected _forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _tan_tensor(a, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    a.backward(dz.mul(a.cos().pow(-2)));
  }
}
registerOperation('tan', Tan);

// reduction

function _sum_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  return new Tensor(
    a.toArray().reduce((acc, val) => acc + val, 0),
    { requires_grad: a.requires_grad },
    { operation: operation }
  );
}

// class generated from unary_op_class("Sum", "sum", backward_operations)
export class Sum extends UnaryOperation {
  private cache: [Tensor];
  protected _forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _sum_tensor(a, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    const result = new Tensor(Array(a.dataLength()).fill(dz.item()));
    a.backward(result);
  }
}
registerOperation('sum', Sum);

function _mean_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  return new Tensor(
    a.toArray().reduce((acc, val) => acc + val, 0) / a.dataLength(),
    { requires_grad: a.requires_grad },
    { operation: operation }
  );
}

// class generated from unary_op_class("Mean", "mean", backward_operations)
export class Mean extends UnaryOperation {
  private cache: [Tensor];
  protected _forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _mean_tensor(a, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    const result = new Tensor(Array(a.dataLength()).fill(dz.item() / a.dataLength()));
    a.backward(result);
  }
}
registerOperation('mean', Mean);

// linalg

function _transpose_tensor(
  a: Tensor,
  dim0: number,
  dim1: number,
  operation: Operation | null = null
): Tensor {
  const output_shape = [...a.shape];
  [output_shape[dim0], output_shape[dim1]] = [output_shape[dim1], output_shape[dim0]];
  const size = a.dataLength();
  const data = new Array(size);

  const a_strides = new Array(a.shape.length);
  const out_strides = new Array(output_shape.length);
  for (let i = a.shape.length - 1, s = 1; i >= 0; i--) {
    a_strides[i] = s;
    s *= a.shape[i];
  }
  for (let i = output_shape.length - 1, s = 1; i >= 0; i--) {
    out_strides[i] = s;
    s *= output_shape[i];
  }

  for(let i=0; i<size; i++) {
    let idx = i;
    let input_idx = 0;
    for (let d = 0; d < output_shape.length; d++) {
      const stride = out_strides[d];
      const coord = Math.floor(idx / stride);
      idx %= stride;

      let input_d = d;
      if (d === dim0) input_d = dim1;
      else if (d === dim1) input_d = dim0;

      input_idx += coord * a_strides[input_d];
    }
    data[i] = a.data[input_idx];
  }

  return new Tensor(
    data,
    { requires_grad: a.requires_grad },
    { operation: operation, shape: output_shape }
  );
}
export class Transpose extends Operation {
  cache: [Tensor, number, number];
  protected _forward(a: Tensor, dim0: number, dim1: number): Tensor {
    this.cache = [a, dim0, dim1];
    return _transpose_tensor(a, dim0, dim1, this);
  }
  protected _backward(dz: Tensor): void {
    const [a, dim0, dim1] = this.cache;

    // backward_operations:
    a.backward(dz.transpose(dim0, dim1));
  }
}
registerOperation('transpose', Transpose);

function _matmul_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  if (a.shape.length == 1 && b.shape.length == 1) {
    return a.mul(b).sum();
  }

  const a_1d = a.shape.length == 1;
  const b_1d = b.shape.length == 1;

  const a_shape = a_1d ? [1, a.shape[0]] : a.shape;
  const b_shape = b_1d ? [b.shape[0], 1] : b.shape;

  if (a_shape[a_shape.length - 1] != b_shape[b_shape.length - 2]) {
    throw new Error('Shape mismatch: ' + a.shape + ' and ' + b.shape);
  }

  const broadcast_shape = _broadcast_shape(a_shape.slice(0, -2), b_shape.slice(0, -2)).concat([
    a_shape[a_shape.length - 2],
    b_shape[b_shape.length - 1]
  ]);

  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);
  const data = new Array(output_size).fill(0);

  const padded_a_shape = _pad_shape(a_shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b_shape, broadcast_shape);

  const dim_M = broadcast_shape[broadcast_shape.length - 2];
  const dim_N = broadcast_shape[broadcast_shape.length - 1];
  const dim_K = a_shape[a_shape.length - 1]; // or b_shape[b_shape.length - 2]

  for (let i = 0; i < output_size; i++) {
    const mn_idx = i % (dim_M * dim_N);
    const m = Math.floor(mn_idx / dim_N);
    const n = mn_idx % dim_N;

    let base_a = _get_original_index(padded_a_shape, broadcast_shape, i - n);
    let base_b = _get_original_index(padded_b_shape, broadcast_shape, i - m * dim_N);

    let sum = 0;
    for(let k=0; k < dim_K; k++) {
      sum += a.data[base_a + k] * b.data[base_b + k * dim_N];
    }
    data[i] = sum;
  }

  let shape_after_removing_extra_dims = [...broadcast_shape];

  if (a_1d) {
    shape_after_removing_extra_dims = shape_after_removing_extra_dims
      .slice(0, -2)
      .concat([broadcast_shape[broadcast_shape.length - 1]]);
  }

  if (b_1d) {
    shape_after_removing_extra_dims = shape_after_removing_extra_dims.slice(0, -1);
  }

  return new Tensor(
    data,
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: shape_after_removing_extra_dims }
  );
}
// class generated from binary_op_class("Matmul", "matmul", backward_operations)
export class Matmul extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return _matmul_tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    
  }
}
registerOperation('matmul', Matmul);

// comparison

const _lt_kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = (a[a_index] < b[b_index]) ? 1 : 0;
  }
  return res;
};

function _lt_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = _lt_kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Lt", "lt", backward_operations)
export class Lt extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return _lt_tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    
  }
}
registerOperation('lt', Lt);

const _gt_kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = (a[a_index] > b[b_index]) ? 1 : 0;
  }
  return res;
};

function _gt_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = _gt_kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Gt", "gt", backward_operations)
export class Gt extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return _gt_tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    
  }
}
registerOperation('gt', Gt);

const _le_kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = (a[a_index] <= b[b_index]) ? 1 : 0;
  }
  return res;
};

function _le_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = _le_kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Le", "le", backward_operations)
export class Le extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return _le_tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    
  }
}
registerOperation('le', Le);

const _ge_kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = (a[a_index] >= b[b_index]) ? 1 : 0;
  }
  return res;
};

function _ge_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = _ge_kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Ge", "ge", backward_operations)
export class Ge extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return _ge_tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    
  }
}
registerOperation('ge', Ge);

const _eq_kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = (a[a_index] == b[b_index]) ? 1 : 0;
  }
  return res;
};

function _eq_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = _eq_kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Eq", "eq", backward_operations)
export class Eq extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return _eq_tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    
  }
}
registerOperation('eq', Eq);

const _ne_kernel = function (a: number[], as: number[], b: number[], bs: number[], bcs: number[], output_size: number) {
  const res = Array(output_size);
  for(let x = 0; x < output_size; x++) {
    const a_index = _get_original_index(as, bcs, x);
    const b_index = _get_original_index(bs, bcs, x);
    res[x] = (a[a_index] != b[b_index]) ? 1 : 0;
  }
  return res;
};

function _ne_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);

  const kernel = _ne_kernel;
  const output_size = broadcast_shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, output_size) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Ne", "ne", backward_operations)
export class Ne extends BinaryOperation {
  private cache: [Tensor, Tensor];
  protected _forward(a: Tensor, b: Tensor): Tensor {
    if (a.requires_grad || b.requires_grad) {
      this.cache = [a, b];
    }
    return _ne_tensor(a, b, a.requires_grad || b.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    
  }
}
registerOperation('ne', Ne);