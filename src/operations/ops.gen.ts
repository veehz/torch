// This file is generated by scripts/generate_script.py from src/operations/ops.ts.j2
import { Tensor } from "../tensor";
import { _broadcast_shape, _get_original_index_from_transposed_index, _get_original_index_kernel, _pad_shape } from "../broadcasting";
import gpu from "../gpu";
import { Operation, BinaryOperation, UnaryOperation } from './base';
import { registerOperation } from "./registry";

function _sum_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  return new Tensor(
    a.data.reduce((acc, val) => acc + val, 0),
    { requires_grad: a.requires_grad },
    { operation: operation }
  );
}

// class generated from unary_op_class("Sum", "sum", backward_operations)
export class Sum extends UnaryOperation {
  private cache: [Tensor];
  public forward(a: Tensor): Tensor {
    this.cache = [a];
    return _sum_tensor(a, this);
  }
  public backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    const result = new Tensor(Array(a.data.length).fill(dz.data[0]));
    a.backward(result);
  }
}
registerOperation('sum', Sum);

function _mean_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  return new Tensor(
    a.data.reduce((acc, val) => acc + val, 0) / a.data.length,
    { requires_grad: a.requires_grad },
    { operation: operation }
  );
}

// class generated from unary_op_class("Mean", "mean", backward_operations)
export class Mean extends UnaryOperation {
  private cache: [Tensor];
  public forward(a: Tensor): Tensor {
    this.cache = [a];
    return _mean_tensor(a, this);
  }
  public backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    const result = new Tensor(Array(a.data.length).fill(dz.data[0] / a.data.length));
    a.backward(result);
  }
}
registerOperation('mean', Mean);

// function generated from binary_op_base("add", "a[a_index] + b[b_index]")
function _add_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);
  const kernel = gpu.createKernel(
    function (a: number[], as: number[], b: number[], bs: number[], bcs: number[]) {
      const a_index = _get_original_index_kernel(as, bcs, this.thread.x);
      const b_index = _get_original_index_kernel(bs, bcs, this.thread.x);

      return a[a_index] + b[b_index];
    },
    {
      constants: {
        shape_length: broadcast_shape.length
      },
      output: [broadcast_shape.reduce((acc, val) => acc * val, 1)]
    }
  );

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Add", "add", backward_operations)
export class Add extends BinaryOperation {
  private cache: [Tensor, Tensor];
  public forward(a: Tensor, b: Tensor): Tensor {
    this.cache = [a, b];
    return _add_tensor(a, b, this);
  }
  public backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    a.backward(dz);
    b.backward(dz);
  }
}
registerOperation('add', Add);

// function generated from binary_op_base("sub", "a[a_index] - b[b_index]")
function _sub_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);
  const kernel = gpu.createKernel(
    function (a: number[], as: number[], b: number[], bs: number[], bcs: number[]) {
      const a_index = _get_original_index_kernel(as, bcs, this.thread.x);
      const b_index = _get_original_index_kernel(bs, bcs, this.thread.x);

      return a[a_index] - b[b_index];
    },
    {
      constants: {
        shape_length: broadcast_shape.length
      },
      output: [broadcast_shape.reduce((acc, val) => acc * val, 1)]
    }
  );

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Sub", "sub", backward_operations)
export class Sub extends BinaryOperation {
  private cache: [Tensor, Tensor];
  public forward(a: Tensor, b: Tensor): Tensor {
    this.cache = [a, b];
    return _sub_tensor(a, b, this);
  }
  public backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    a.backward(dz);
    b.backward(dz.mul(new Tensor(-1)));
  }
}
registerOperation('sub', Sub);

// function generated from binary_op_base("mul", "a[a_index] * b[b_index]")
function _mul_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);
  const kernel = gpu.createKernel(
    function (a: number[], as: number[], b: number[], bs: number[], bcs: number[]) {
      const a_index = _get_original_index_kernel(as, bcs, this.thread.x);
      const b_index = _get_original_index_kernel(bs, bcs, this.thread.x);

      return a[a_index] * b[b_index];
    },
    {
      constants: {
        shape_length: broadcast_shape.length
      },
      output: [broadcast_shape.reduce((acc, val) => acc * val, 1)]
    }
  );

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Mul", "mul", backward_operations)
export class Mul extends BinaryOperation {
  private cache: [Tensor, Tensor];
  public forward(a: Tensor, b: Tensor): Tensor {
    this.cache = [a, b];
    return _mul_tensor(a, b, this);
  }
  public backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    a.backward(dz.mul(b));
    b.backward(dz.mul(a));
  }
}
registerOperation('mul', Mul);

// function generated from binary_op_base("div", "a[a_index] / b[b_index]")
function _div_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);
  const kernel = gpu.createKernel(
    function (a: number[], as: number[], b: number[], bs: number[], bcs: number[]) {
      const a_index = _get_original_index_kernel(as, bcs, this.thread.x);
      const b_index = _get_original_index_kernel(bs, bcs, this.thread.x);

      return a[a_index] / b[b_index];
    },
    {
      constants: {
        shape_length: broadcast_shape.length
      },
      output: [broadcast_shape.reduce((acc, val) => acc * val, 1)]
    }
  );

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Div", "div", backward_operations)
export class Div extends BinaryOperation {
  private cache: [Tensor, Tensor];
  public forward(a: Tensor, b: Tensor): Tensor {
    this.cache = [a, b];
    return _div_tensor(a, b, this);
  }
  public backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    a.backward(dz.div(b));
    b.backward(dz.mul(a).mul(new Tensor(-1)).div(b).div(b));
  }
}
registerOperation('div', Div);

// function generated from binary_op_base("pow", "a[a_index] ** b[b_index]")
function _pow_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  const broadcast_shape = _broadcast_shape(a.shape, b.shape);
  const padded_a_shape = _pad_shape(a.shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b.shape, broadcast_shape);
  const kernel = gpu.createKernel(
    function (a: number[], as: number[], b: number[], bs: number[], bcs: number[]) {
      const a_index = _get_original_index_kernel(as, bcs, this.thread.x);
      const b_index = _get_original_index_kernel(bs, bcs, this.thread.x);

      return a[a_index] ** b[b_index];
    },
    {
      constants: {
        shape_length: broadcast_shape.length
      },
      output: [broadcast_shape.reduce((acc, val) => acc * val, 1)]
    }
  );

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: broadcast_shape }
  );
}
// class generated from binary_op_class("Pow", "pow", backward_operations)
export class Pow extends BinaryOperation {
  private cache: [Tensor, Tensor];
  public forward(a: Tensor, b: Tensor): Tensor {
    this.cache = [a, b];
    return _pow_tensor(a, b, this);
  }
  public backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    a.backward(dz.mul(b).mul(a.pow(b.sub(new Tensor(1)))));
    b.backward(dz.mul(a.pow(b)).mul(a.log()));
  }
}
registerOperation('pow', Pow);

function _transpose_tensor(a: Tensor, dim0: number, dim1: number, operation: Operation | null = null): Tensor {
  const kernel = gpu.createKernel(
    function (a: number[], as: number[], dim0: number, dim1: number) {
      const a_index = _get_original_index_from_transposed_index(as, dim0, dim1, this.thread.x);
      return a[a_index];
    },
    {
      constants: {
        shape_length: a.shape.length
      },
      output: [a.shape.reduce((acc, val) => acc * val, 1)]
    }
  );

  const swapped_shape = a.shape;
  [swapped_shape[dim0], swapped_shape[dim1]] = [swapped_shape[dim1], swapped_shape[dim0]];

  return new Tensor(
    kernel(a.data, a.shape, dim0, dim1) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: swapped_shape }
  );
}

export class Transpose extends Operation {
  cache: [Tensor, number, number];
  forward(a: Tensor, dim0: number, dim1: number): Tensor {
    this.cache = [a, dim0, dim1];
    return _transpose_tensor(a, dim0, dim1, this);
  }
  backward(dz: Tensor): void {
    const [a, dim0, dim1] = this.cache;

    // backward_operations:
    a.backward(dz.transpose(dim0, dim1));
  }
}
registerOperation('transpose', Transpose);

function _matmul_tensor(a: Tensor, b: Tensor, operation: Operation | null = null): Tensor {
  if (a.shape.length == 1 && b.shape.length == 1) {
    return a.mul(b).sum();
  }

  const a_1d = a.shape.length == 1;
  const b_1d = b.shape.length == 1;

  const a_shape = a_1d ? [1, a.shape[0]] : a.shape;
  const b_shape = b_1d ? [b.shape[0], 1] : b.shape;

  if(a_shape[a_shape.length - 1] != b_shape[b_shape.length - 2]) {
    // TODO: check what error pytorch throws
    throw new Error("Shape mismatch: " + a.shape + " and " + b.shape);
  }

  const loop_iterations = a_shape[a_shape.length - 1];

  if(loop_iterations > 1000) {
    // TODO: can try fixing with maxLoopIterations by gpu.js
    throw new Error("Loop iterations too large: " + loop_iterations);
  }

  const broadcast_shape = _broadcast_shape(a_shape.slice(0, -2), b_shape.slice(0, -2))
                            .concat([a_shape[a_shape.length - 2], b_shape[b_shape.length - 1]]);

  const padded_a_shape = _pad_shape(a_shape, broadcast_shape);
  const padded_b_shape = _pad_shape(b_shape, broadcast_shape);

  function _matmul_kernel(a: number[], as: number[], b: number[], bs: number[], bcs: number[], lp: number) {
    let a_index = _get_original_index_kernel(as, bcs, this.thread.x);
    let b_index = _get_original_index_kernel(bs, bcs, this.thread.x);

    const l = this.constants.shape_length;

    const position = this.thread.x % (bcs[l] * bcs[l + 1]);
    a_index = a_index * as[l] * as[l + 1] + Math.floor(position / bcs[l + 1]) * as[l + 1];
    b_index = b_index * bs[l] * bs[l + 1] + position % bcs[l + 1];

    const b_stride = bs[l+1];

    let sum = 0;
    for(let i = 0; i < lp; i++) {
      sum = sum + a[a_index] * b[b_index];
      a_index = a_index + 1;
      b_index = b_index + b_stride;
    }

    return sum;
  }

  const kernel = gpu.createKernel(
    _matmul_kernel,
    {
      constants: {
        // assumes that _get_original_index_kernel reads from the front
        shape_length: broadcast_shape.length - 2
      },
      output: [broadcast_shape.reduce((acc, val) => acc * val, 1)],
    }
  );

  let shape_after_removing_extra_dims = broadcast_shape;

  if(a_1d) {
    shape_after_removing_extra_dims = shape_after_removing_extra_dims.slice(0, -2).concat([a.shape[0]]);
  }

  if(b_1d) {
    shape_after_removing_extra_dims = shape_after_removing_extra_dims.slice(0, -1);
  }

  return new Tensor(
    kernel(a.data, padded_a_shape, b.data, padded_b_shape, broadcast_shape, loop_iterations) as number[],
    { requires_grad: a.requires_grad || b.requires_grad },
    { operation: operation, shape: shape_after_removing_extra_dims }
  );
}

// class generated from binary_op_class("Matmul", "matmul", backward_operations)
export class Matmul extends BinaryOperation {
  private cache: [Tensor, Tensor];
  public forward(a: Tensor, b: Tensor): Tensor {
    this.cache = [a, b];
    return _matmul_tensor(a, b, this);
  }
  public backward(dz: Tensor): void {
    const [a, b] = this.cache;

    // backward_operations:
    
  }
}
registerOperation('matmul', Matmul);



// function generated from unary_op_base("log", "Math.log(a[this.thread.x])")
function _log_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = gpu.createKernel(
    function (a: number[]) {
      return Math.log(a[this.thread.x]);
    },
    {
      output: [a.shape.reduce((acc, val) => acc * val, 1)]
    }
  );

  return new Tensor(
    kernel(a.data) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Log", "log", backward_operations)
export class Log extends UnaryOperation {
  private cache: [Tensor];
  public forward(a: Tensor): Tensor {
    this.cache = [a];
    return _log_tensor(a, this);
  }
  public backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    a.backward(new Tensor(1).div(a));
  }
}
registerOperation('log', Log);

