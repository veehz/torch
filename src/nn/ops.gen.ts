// This file is generated by scripts/generate_script.py from src/nn/ops.ts.j2
import { Tensor } from '../tensor';
import {
  _broadcast_shape,
  _get_original_index_from_transposed_index,
  _get_original_index_kernel,
  _pad_shape
} from '../broadcasting';
import gpu from '../gpu';
import { Operation, BinaryOperation, UnaryOperation } from '../operations/base';
import { registerOperation } from '../operations/registry';

// function generated from unary_op_base("relu", "Math.max(a[this.thread.x], 0)")

const _relu_kernel = gpu.createKernel(
  function (a: number[]) {
    return Math.max(a[this.thread.x], 0);
  },
  {
    dynamicOutput: true,
    dynamicArguments: true,
    // pipeline: true,
    // immutable: true
  }
);

function _relu_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = _relu_kernel;
  kernel.setOutput([a.shape.reduce((acc, val) => acc * val, 1)]);

  return new Tensor(
    kernel(a.data) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Relu", "relu", backward_operations)
export class Relu extends UnaryOperation {
  private cache: [Tensor];
  public forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _relu_tensor(a, a.requires_grad ? this : null);
  }
  public backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    a.backward(dz.mul(a.gt(0)));
  }
}
registerOperation('relu', Relu);

// function generated from unary_op_base("sigmoid", "1 / (1 + Math.exp(-a[this.thread.x]))")

const _sigmoid_kernel = gpu.createKernel(
  function (a: number[]) {
    return 1 / (1 + Math.exp(-a[this.thread.x]));
  },
  {
    dynamicOutput: true,
    dynamicArguments: true,
    // pipeline: true,
    // immutable: true
  }
);

function _sigmoid_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = _sigmoid_kernel;
  kernel.setOutput([a.shape.reduce((acc, val) => acc * val, 1)]);

  return new Tensor(
    kernel(a.data) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Sigmoid", "sigmoid", backward_operations)
export class Sigmoid extends UnaryOperation {
  private cache: [Tensor];
  public forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _sigmoid_tensor(a, a.requires_grad ? this : null);
  }
  public backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    a.backward(dz.mul(a.exp().add(1).pow(-2).reciprocal().mul(a.exp()).mul(-1)));
  }
}
registerOperation('sigmoid', Sigmoid);