// This file is generated by scripts/generate_script.py from src/nn/ops.ts.j2
import { Tensor } from '../tensor';
import {
  _broadcast_shape,
  _get_original_index_from_transposed_index,
  _get_original_index_kernel,
  _pad_shape
} from '../broadcasting';
import gpu from '../gpu';
import { Operation, BinaryOperation, UnaryOperation } from '../operations/base';
import { registerOperation } from '../operations/registry';

// function generated from unary_op_base("relu", "Math.max(a[this.thread.x], 0)")
function _relu_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = gpu.createKernel(
    function (a: number[]) {
      return Math.max(a[this.thread.x], 0);
    },
    {
      output: [a.shape.reduce((acc, val) => acc * val, 1)]
    }
  );

  return new Tensor(
    kernel(a.data) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Relu", "relu", backward_operations)
export class Relu extends UnaryOperation {
  private cache: [Tensor];
  public forward(a: Tensor): Tensor {
    this.cache = [a];
    return _relu_tensor(a, this);
  }
  public backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    a.backward(dz.mul(a.gt(0)));
  }
}
registerOperation('relu', Relu);