// This file is generated by scripts/generate_script.py from src/nn/ops.ts.j2
import { Tensor } from '../tensor';
import {
  _broadcast_shape,
  _get_original_index_from_transposed_index,
  _get_original_index_kernel,
  _pad_shape
} from '../broadcasting';
import { Operation, BinaryOperation, UnaryOperation } from '../operations/base';
import { registerOperation } from '../operations/registry';

// function generated from unary_op_base("relu", "Math.max(a[x], 0)")

const _relu_kernel = function (a: number[], output: number) {
  const res = new Array(output);
  for (let x = 0; x < output; x++) {
    res[x] = Math.max(a[x], 0);
  }
  return res;
};

function _relu_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = _relu_kernel;
  const output = a.shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, output) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Relu", "relu", backward_operations)
export class Relu extends UnaryOperation {
  private cache: [Tensor];
  protected _forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _relu_tensor(a, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    a.backward(dz.mul(a.gt(0)));
  }
}
registerOperation('relu', Relu);

// function generated from unary_op_base("sigmoid", "1 / (1 + Math.exp(-a[x]))")

const _sigmoid_kernel = function (a: number[], output: number) {
  const res = new Array(output);
  for (let x = 0; x < output; x++) {
    res[x] = 1 / (1 + Math.exp(-a[x]));
  }
  return res;
};

function _sigmoid_tensor(a: Tensor, operation: Operation | null = null): Tensor {
  const kernel = _sigmoid_kernel;
  const output = a.shape.reduce((acc, val) => acc * val, 1);

  return new Tensor(
    kernel(a.data, output) as number[],
    { requires_grad: a.requires_grad },
    { operation: operation, shape: a.shape }
  );
}
// class generated from unary_op_class("Sigmoid", "sigmoid", backward_operations)
export class Sigmoid extends UnaryOperation {
  private cache: [Tensor];
  protected _forward(a: Tensor): Tensor {
    if (a.requires_grad) {
      this.cache = [a];
    }
    return _sigmoid_tensor(a, a.requires_grad ? this : null);
  }
  protected _backward(dz: Tensor): void {
    const [a] = this.cache;

    // backward_operations:
    a.backward(dz.mul(a.exp().add(1).pow(-2).reciprocal().mul(a.exp()).mul(-1)));
  }
}
registerOperation('sigmoid', Sigmoid);